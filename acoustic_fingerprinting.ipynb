{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic fingerprinting\n",
    "\n",
    "The solution will consists of three separate components:\n",
    "1. Create fingerprints of all songs in the database: Code that creates an unique audio fingerprint of each song in the database\n",
    "2. Take FFT of a short audio snippet : Code that makes Fourier transforms of a short snippet\n",
    "3. Matching algorithm: Algorithm that has two functions:\n",
    "<br>\n",
    "    3.1. Matches the FFT of the short audio snippet to each of the fingerprints in the database\n",
    "<br>\n",
    "    3.2. Returns a list of songs from the database whose fingerprints are closely matching to the short audio snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import blackman, hanning, hamming\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of points to use in the FFT\n",
    "n_fft = 512\n",
    "\n",
    "# Frame size and overlap between frames for the FFT\n",
    "frame_size = 0.025\n",
    "frame_overlap = 0.015\n",
    "\n",
    "# Location of the songs\n",
    "songs_location = \"C:/Users/bre49823/Google Drive/MusicEngine/wav/\" # \"/Users/valentin/Documents/MusicEngine/wav/test/\"\n",
    "songs_list = os.listdir(songs_location)\n",
    "\n",
    "# Location of sample\n",
    "sample_file = \"C:/Users/bre49823/Google Drive/MusicEngine/wav/dt_16bars_102rap.wav\" # \"/Users/valentin/Documents/MusicEngine/wav/test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Create acoustic fingerprints for all songs in the database\n",
    "########################################\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "song_fft_window = []\n",
    "band_0_25 = []\n",
    "band_26_50 = []\n",
    "band_51_75 = []\n",
    "band_76_100 = []\n",
    "band_101_125 = []\n",
    "band_126_150 = []\n",
    "band_151_175 = []\n",
    "band_176_200 = []\n",
    "band_201_225 = []\n",
    "band_226_250 = []\n",
    "\n",
    "\n",
    "### Loop through all songs in the database for which I want to create a fingerprint\n",
    "for s in range(0, len(songs_list)):\n",
    "    \n",
    "    #### Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "    sample_rate, soundtrack_data = wavfile.read(songs_location + songs_list[s])\n",
    "\n",
    "    # If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "    if len(soundtrack_data.shape) == 2:\n",
    "        audio_signal = soundtrack_data.T[0]\n",
    "        audio_signal = audio_signal[0:int(2 * sample_rate)]       # keep only the first n seconds\n",
    "    else:\n",
    "        audio_signal = soundtrack_data\n",
    "        audio_signal = audio_signal[0:int(2 * sample_rate)]\n",
    "\n",
    "    time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "    #### Apply pre-emphasis filter to the raw audio data\n",
    "    emphasis_coeff = 0.95\n",
    "    audio_signal = np.append(audio_signal[0], audio_signal[1:] - emphasis_coeff * audio_signal[:-1])\n",
    "\n",
    "\n",
    "    #### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "    # Calculate the length of each frame and the step for moving forward the FFT\n",
    "    frame_stride = round(frame_size - frame_overlap, 3)\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "    # Calculate the total number of frames\n",
    "    signal_length = len(audio_signal)\n",
    "    number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    # Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "    pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "    zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "    pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "\n",
    "    #### frames_idx is an index which is used to split the pad_signal array\n",
    "    frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "    signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "    #### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "    signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "    #### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "    # The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "    signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "    signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "    # Calculate the power for each frame\n",
    "    signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "    \n",
    "    #### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "    # When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "    npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "    frequency_bins = 10\n",
    "    points_per_bin = npoints / frequency_bins\n",
    "\n",
    "    #### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "    frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "    # Loop through all the frames/windows to which Fourier transform was applied\n",
    "    for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "        # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "        fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "        # Split the results from the Fourier transform into the frequency bins created above\n",
    "        fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "        # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "        max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "        # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "        band_0_25.append(max_power[0])\n",
    "        band_26_50.append(max_power[1])\n",
    "        band_51_75.append(max_power[2])\n",
    "        band_76_100.append(max_power[3])\n",
    "        band_101_125.append(max_power[4])\n",
    "        band_126_150.append(max_power[5])\n",
    "        band_151_175.append(max_power[6])\n",
    "        band_176_200.append(max_power[7])\n",
    "        band_201_225.append(max_power[8])\n",
    "        band_226_250.append(max_power[9])\n",
    "\n",
    "        # Create an index which is a combination of song name and Fourier transform frame. This index tracks songs and frame sequence\n",
    "        # The number of records in this list should equal the number of records in the lists with frquency bands\n",
    "        fft_window = i            # A sequential number of the Fourier transform windows for each song\n",
    "        song_fft_window.append(songs_list[s].split(\".wav\")[0] + \"_\" + str(fft_window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Create an acoustic fingerprint for the short song sample \n",
    "########################################\n",
    "\n",
    "#### Read in the raw audio for the sample\n",
    "sample_rate, soundtrack_data = wavfile.read(sample_file)\n",
    "audio_signal = soundtrack_data.T[0]                     # this is a two channel soundtrack, get only one of the tracks\n",
    "audio_signal = audio_signal[0:int(2 * sample_rate)]       # keep only the first n seconds\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "song_fft_window = []\n",
    "band_0_25 = []\n",
    "band_26_50 = []\n",
    "band_51_75 = []\n",
    "band_76_100 = []\n",
    "band_101_125 = []\n",
    "band_126_150 = []\n",
    "band_151_175 = []\n",
    "band_176_200 = []\n",
    "band_201_225 = []\n",
    "band_226_250 = []\n",
    "\n",
    "\n",
    "### Loop through all songs in the database for which I want to create a fingerprint\n",
    "for s in range(0, len(songs_list)):\n",
    "    \n",
    "    #### Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "    sample_rate, soundtrack_data = wavfile.read(songs_location + songs_list[s])\n",
    "\n",
    "    # If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "    if len(soundtrack_data.shape) == 2:\n",
    "        audio_signal = soundtrack_data.T[0]\n",
    "        audio_signal = audio_signal[0:int(2 * sample_rate)]       # keep only the first n seconds\n",
    "    else:\n",
    "        audio_signal = soundtrack_data\n",
    "        audio_signal = audio_signal[0:int(2 * sample_rate)]\n",
    "\n",
    "    time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "    #### Apply pre-emphasis filter to the raw audio data\n",
    "    emphasis_coeff = 0.95\n",
    "    audio_signal = np.append(audio_signal[0], audio_signal[1:] - emphasis_coeff * audio_signal[:-1])\n",
    "\n",
    "\n",
    "    #### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "    # Calculate the length of each frame and the step for moving forward the FFT\n",
    "    frame_stride = round(frame_size - frame_overlap, 3)\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "    # Calculate the total number of frames\n",
    "    signal_length = len(audio_signal)\n",
    "    number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    # Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "    pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "    zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "    pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "\n",
    "    #### frames_idx is an index which is used to split the pad_signal array\n",
    "    frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "    signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "    #### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "    signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "    #### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "    # The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "    signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "    signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "    # Calculate the power for each frame\n",
    "    signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "    \n",
    "    #### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "    # When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "    npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "    frequency_bins = 10\n",
    "    points_per_bin = npoints / frequency_bins\n",
    "\n",
    "    #### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "    frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "    # Loop through all the frames/windows to which Fourier transform was applied\n",
    "    for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "        # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "        fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "        # Split the results from the Fourier transform into the frequency bins created above\n",
    "        fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "        # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "        max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "        # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "        band_0_25.append(max_power[0])\n",
    "        band_26_50.append(max_power[1])\n",
    "        band_51_75.append(max_power[2])\n",
    "        band_76_100.append(max_power[3])\n",
    "        band_101_125.append(max_power[4])\n",
    "        band_126_150.append(max_power[5])\n",
    "        band_151_175.append(max_power[6])\n",
    "        band_176_200.append(max_power[7])\n",
    "        band_201_225.append(max_power[8])\n",
    "        band_226_250.append(max_power[9])\n",
    "\n",
    "        # Create an index which is a combination of song name and Fourier transform frame. This index tracks songs and frame sequence\n",
    "        # The number of records in this list should equal the number of records in the lists with frquency bands\n",
    "        fft_window = i            # A sequential number of the Fourier transform windows for each song\n",
    "        song_fft_window.append(songs_list[s].split(\".wav\")[0] + \"_\" + str(fft_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Match the fingerprint of the sample to the database and return the results\n",
    "# Determine which match is the actual\n",
    "########################################\n",
    "\n",
    "# These are test samples\n",
    "sample_0_25 = band_0_25[0]\n",
    "sample_26_50 = band_26_50[0]\n",
    "sample_51_75 = band_51_75[0]\n",
    "sample_76_100 = band_76_100[0]\n",
    "sample_101_125 = band_101_125[0]\n",
    "sample_126_150 = band_126_150[0]\n",
    "sample_151_175 = band_151_175[0]\n",
    "sample_176_200 = band_176_200[0]\n",
    "sample_201_225 = band_201_225[0]\n",
    "sample_226_250 = band_226_250[0]\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "t = sample_26_50 == band_26_50\n",
    "match_idx = list(compress(xrange(len(t)), t))\n",
    "\n",
    "matched_songs = [song_fft_window[i] for i in list(compress(xrange(len(t)), t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bach_partita_e_major_0']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-137-6bac6506a7ca>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-137-6bac6506a7ca>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for (i = 0; i < 3; i++) {\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "for (i = 0; i < 3; i++) {\n",
    "  x += 5 * i\n",
    "}\n",
    "console.log(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
