{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic fingerprinting\n",
    "\n",
    "The solution will consists of three separate components:\n",
    "1. Create fingerprints of all songs in the database: Code that creates an unique audio fingerprint of each song in the database\n",
    "2. Take FFT of a short audio snippet : Code that makes Fourier transforms of a short snippet\n",
    "3. Matching algorithm: Algorithm that has two functions:\n",
    "<br>\n",
    "    3.1. Matches the FFT of the short audio snippet to each of the fingerprints in the database\n",
    "<br>\n",
    "    3.2. Returns a list of songs from the database whose fingerprints are closely matching to the short audio snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import blackman, hanning, hamming\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of points to use in the FFT\n",
    "n_fft = 512\n",
    "\n",
    "# Frame size and overlap between frames for the FFT\n",
    "frame_size = 0.025\n",
    "frame_overlap = 0.015\n",
    "\n",
    "# Location of the songs\n",
    "songs_location = \"C:/Users/bre49823/Google Drive/MusicEngine/wav/\" # \"/Users/valentin/Documents/MusicEngine/wav/test/\"\n",
    "songs_list = os.listdir(songs_location)\n",
    "\n",
    "# Location of sample\n",
    "sample_file = \"C:/Users/bre49823/Google Drive/MusicEngine/sample/sampleDt16bars102rap.wav\" # \"/Users/valentin/Documents/MusicEngine/wav/test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Create acoustic fingerprints for all songs in the database\n",
    "########################################\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "song_fft_window = []\n",
    "band_0_25 = []\n",
    "band_26_50 = []\n",
    "band_51_75 = []\n",
    "band_76_100 = []\n",
    "band_101_125 = []\n",
    "band_126_150 = []\n",
    "band_151_175 = []\n",
    "band_176_200 = []\n",
    "band_201_225 = []\n",
    "band_226_250 = []\n",
    "\n",
    "\n",
    "### Loop through all songs in the database for which I want to create a fingerprint\n",
    "for s in range(0, len(songs_list)):\n",
    "    \n",
    "    #### Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "    sample_rate, soundtrack_data = wavfile.read(songs_location + songs_list[s])\n",
    "\n",
    "    # If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "    if len(soundtrack_data.shape) == 2:\n",
    "        audio_signal = soundtrack_data.T[0]\n",
    "        audio_signal = audio_signal[0:int(5 * sample_rate)]       # keep only the first n seconds\n",
    "    else:\n",
    "        audio_signal = soundtrack_data\n",
    "        audio_signal = audio_signal[0:int(5 * sample_rate)]\n",
    "\n",
    "    time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "    #### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "    # Calculate the length of each frame and the step for moving forward the FFT\n",
    "    frame_stride = round(frame_size - frame_overlap, 3)\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "    # Calculate the total number of frames\n",
    "    signal_length = len(audio_signal)\n",
    "    number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    # Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "    pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "    zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "    pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "\n",
    "    #### frames_idx is an index which is used to split the pad_signal array\n",
    "    frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "    signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "    #### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "    signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "    #### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "    # The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "    signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "    signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "    # Calculate the power for each frame\n",
    "    signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "    \n",
    "    #### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "    # When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "    npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "    frequency_bins = 10\n",
    "    points_per_bin = npoints / frequency_bins\n",
    "\n",
    "    #### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "    frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "    # Loop through all the frames/windows to which Fourier transform was applied\n",
    "    for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "        # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "        fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "        # Split the results from the Fourier transform into the frequency bins created above\n",
    "        fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "        # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "        max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "        # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "        band_0_25.append(max_power[0])\n",
    "        band_26_50.append(max_power[1])\n",
    "        band_51_75.append(max_power[2])\n",
    "        band_76_100.append(max_power[3])\n",
    "        band_101_125.append(max_power[4])\n",
    "        band_126_150.append(max_power[5])\n",
    "        band_151_175.append(max_power[6])\n",
    "        band_176_200.append(max_power[7])\n",
    "        band_201_225.append(max_power[8])\n",
    "        band_226_250.append(max_power[9])\n",
    "\n",
    "        # Create an index which is a combination of song name and Fourier transform frame. This index tracks songs and frame sequence\n",
    "        # The number of records in this list should equal the number of records in the lists with frquency bands\n",
    "        fft_window = i            # A sequential number of the Fourier transform windows for each song\n",
    "        song_fft_window.append(songs_list[s].split(\".wav\")[0] + \"_\" + str(fft_window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Create an acoustic fingerprint for the short song sample \n",
    "########################################\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "sample_0_25 = []\n",
    "sample_26_50 = []\n",
    "sample_51_75 = []\n",
    "sample_76_100 = []\n",
    "sample_101_125 = []\n",
    "sample_126_150 = []\n",
    "sample_151_175 = []\n",
    "sample_176_200 = []\n",
    "sample_201_225 = []\n",
    "sample_226_250 = []\n",
    "\n",
    "\n",
    "#### Read in the raw audio for the sample\n",
    "sample_rate, soundtrack_data = wavfile.read(sample_file)\n",
    "\n",
    "# If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "if len(soundtrack_data.shape) == 2:\n",
    "    audio_signal = soundtrack_data.T[0]\n",
    "    audio_signal = audio_signal[0:int(2 * sample_rate)]       # keep only the first n seconds\n",
    "else:\n",
    "    audio_signal = soundtrack_data\n",
    "    audio_signal = audio_signal[0:int(2 * sample_rate)]\n",
    "\n",
    "time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "#### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "# Calculate the length of each frame and the step for moving forward the FFT\n",
    "frame_stride = round(frame_size - frame_overlap, 3)\n",
    "frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "# Calculate the total number of frames\n",
    "signal_length = len(audio_signal)\n",
    "number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "# Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "\n",
    "#### frames_idx is an index which is used to split the pad_signal array\n",
    "frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "#### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "#### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "# The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "# Calculate the power for each frame\n",
    "signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "\n",
    "#### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "# When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "frequency_bins = 10\n",
    "points_per_bin = npoints / frequency_bins\n",
    "\n",
    "#### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "# Loop through all the frames/windows to which Fourier transform was applied\n",
    "for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "    # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "    fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "    # Split the results from the Fourier transform into the frequency bins created above\n",
    "    fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "    # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "    max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "    # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "    sample_0_25.append(max_power[0])\n",
    "    sample_26_50.append(max_power[1])\n",
    "    sample_51_75.append(max_power[2])\n",
    "    sample_76_100.append(max_power[3])\n",
    "    sample_101_125.append(max_power[4])\n",
    "    sample_126_150.append(max_power[5])\n",
    "    sample_151_175.append(max_power[6])\n",
    "    sample_176_200.append(max_power[7])\n",
    "    sample_201_225.append(max_power[8])\n",
    "    sample_226_250.append(max_power[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 6, 7, 8]\n",
      "['numberOfTheBeast', 'mariana', 'mariana', 'mariana', 'mariana', 'ledenoMomiche']\n",
      "{'ledenoMomiche': 1, 'numberOfTheBeast': 1, 'mariana': 4}\n",
      "[1, 4, 5, 6, 7, 8, 9]\n",
      "['numberOfTheBeast', 'mariana', 'mariana', 'mariana', 'mariana', 'ledenoMomiche', 'ledenoMomiche']\n",
      "{'ledenoMomiche': 2, 'numberOfTheBeast': 1, 'mariana': 4}\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Match the fingerprint of the sample to the database and return the results\n",
    "# Determine which match is the actual\n",
    "########################################\n",
    "\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "final_match_df = pd.DataFrame(columns = [\"song\"])\n",
    "\n",
    "sample_0_25 = set([1, 2, 3, 4])\n",
    "sample_26_50 = set([5, 6, 7, 8])\n",
    "\n",
    "band_0_25 = [1, 2, 3, 4, 100, 200, 3, 1, 100, 2, 3, 2, 1, 2, 3, 4, 100, 1, 2, 3, 100, 100, 100, 100]\n",
    "band_26_50 = [5, 6, 7, 8, 100, 200, 7, 5, 100, 6, 7, 6, 5, 6, 7, 8, 100, 5, 6, 7, 100, 100, 100, 100]\n",
    "\n",
    "song = [\"numberOfTheBeast_1\", \"numberOfTheBeast_2\", \"numberOfTheBeast_3\", \"numberOfTheBeast_4\",\n",
    "        \"mariana_1\", \"mariana_2\", \"mariana_3\", \"mariana_4\",\n",
    "        \"ledenoMomiche_1\", \"ledenoMomiche_2\", \"ledenoMomiche_3\", \"ledenoMomiche_4\",\n",
    "        \"btr_1\", \"btr_2\", \"btr_3\", \"btr_4\",\n",
    "        \"d2_1\", \"d2_2\", \"d2_3\", \"d2_4\",\n",
    "        \"shturcite_1\", \"shturcite_2\", \"shturcite_3\", \"shturcite_4\"]\n",
    "\n",
    "sample_list = [\"sample_0_25\", \"sample_26_50\"]\n",
    "database_list = [\"band_0_25\", \"band_26_50\"]\n",
    "\n",
    "'''\n",
    "sample_list = [\"sample_0_25\", \"sample_26_50\", \"sample_51_75\", \"sample_76_100\", \"sample_101_125\",\n",
    "               \"sample_126_150\", \"sample_151_175\", \"sample_176_200\", \"sample_201_225\", \"sample_226_250\"]\n",
    "database_list = [\"band_0_25\", \"band_26_50\", \"band_51_75\", \"band_76_100\", \"band_101_125\",\n",
    "                 \"band_126_150\", \"band_151_175\", \"band_176_200\", \"band_201_225\", \"band_226_250\"]\n",
    "'''\n",
    "\n",
    "for j in range(0, len(sample_list)):\n",
    "\n",
    "    match_idx = [i for i, item in enumerate(database_list[j]) if item in sample_list[j]]\n",
    "    print(match_idx)\n",
    "    \n",
    "    song_mach_list = [song[i].split(\"_\")[0] for i in match_idx]\n",
    "    print(song_mach_list)\n",
    "    \n",
    "    count_match_occurences = dict(Counter(song_mach_list))\n",
    "    print(count_match_occurences)\n",
    "  #  sorted_match_occurences = sorted(count_match_occurences.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    " #   summary_match_df = pd.DataFrame.from_records(sorted_match_occurences, [\"song\", sample_list[j]])\n",
    " #   final_match_df = pd.merge(final_match_df, summary_match_df, on = \"song\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('numberOfTheBeast', 4),\n",
       " ('btr', 4),\n",
       " ('ledenoMomiche', 3),\n",
       " ('d2', 3),\n",
       " ('mariana', 2)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = [\"song\"])\n",
    "\n",
    "sample = set([1, 2, 3, 4])\n",
    "band = [1, 2, 3, 4, 100, 200, 3, 1, 100, 2, 3, 2, 1, 2, 3, 4, 100, 1, 2, 3, 100, 100, 100, 100]\n",
    "\n",
    "song = [\"numberOfTheBeast_1\", \"numberOfTheBeast_2\", \"numberOfTheBeast_3\", \"numberOfTheBeast_4\",\n",
    "        \"mariana_1\", \"mariana_2\", \"mariana_3\", \"mariana_4\",\n",
    "        \"ledenoMomiche_1\", \"ledenoMomiche_2\", \"ledenoMomiche_3\", \"ledenoMomiche_4\",\n",
    "        \"btr_1\", \"btr_2\", \"btr_3\", \"btr_4\",\n",
    "        \"d2_1\", \"d2_2\", \"d2_3\", \"d2_4\",\n",
    "        \"shturcite_1\", \"shturcite_2\", \"shturcite_3\", \"shturcite_4\"]\n",
    "\n",
    "match_idx = [i for i, item in enumerate(band) if item in sample]\n",
    "count_match_occurences = dict(Counter([song[i].split(\"_\")[0] for i in match_idx]))\n",
    "sorted_match_occurences = sorted(count_match_occurences.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "summary_match_df = pd.DataFrame.from_records(sorted_match_occurences, columns = [\"song\", \"band\"])\n",
    "sorted_match_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>band</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>numberOfTheBeast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>btr</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ledenoMomiche</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mariana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               song  band\n",
       "0  numberOfTheBeast     4\n",
       "1               btr     4\n",
       "2     ledenoMomiche     3\n",
       "3                d2     3\n",
       "4           mariana     2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [song, sample_0_25]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_26_50]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_51_75]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_76_100]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_101_125]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_126_150]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_151_175]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_176_200]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_201_225]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [song, sample_226_250]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "sample_list = [\"sample_0_25\", \"sample_26_50\", \"sample_51_75\", \"sample_76_100\", \"sample_101_125\",\n",
    "               \"sample_126_150\", \"sample_151_175\", \"sample_176_200\", \"sample_201_225\", \"sample_226_250\"]\n",
    "database_list = [\"band_0_25\", \"band_26_50\", \"band_51_75\", \"band_76_100\", \"band_101_125\",\n",
    "                 \"band_126_150\", \"band_151_175\", \"band_176_200\", \"band_201_225\", \"band_226_250\"]\n",
    "\n",
    "df = pd.DataFrame(columns = [\"song\", \"band_number\"])\n",
    "\n",
    "for j in range(0, len(sample_list)):\n",
    "    df.columns = [\"song\", sample_list[j]]\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
