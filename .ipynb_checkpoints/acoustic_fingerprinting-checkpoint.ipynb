{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic fingerprinting\n",
    "This program serves two purposes:\n",
    "<br>\n",
    "1) Creates unique acoustic fingerprints of all songs in a database\n",
    "<br>\n",
    "2) Matches a short song sample to the complete particular song in the database\n",
    "\n",
    "It's a work in progress... and needs to be factorized and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import blackman, hanning, hamming\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of points to use in the FFT\n",
    "n_fft = 512\n",
    "\n",
    "# Frame size and overlap between frames for the FFT\n",
    "frame_size = 0.025\n",
    "frame_overlap = 0.015\n",
    "\n",
    "# Create a list with songs. Remove certain elements from the list\n",
    "songs_location = \"C:/Users/bre49823/GoogleDrive/MusicEngine/wav/\" # \"/Users/valentin/GoogleDrive/MusicEngine/wav/\"\n",
    "songs_list = os.listdir(songs_location)\n",
    "\n",
    "try:\n",
    "    songs_list.remove('.DS_Store')\n",
    "except:\n",
    "    songs_list\n",
    "\n",
    "\n",
    "# Location of sample\n",
    "#sample_file = \"C:/Users/bre49823/GoogleDrive/MusicEngine/sample/sampleDt16bars102rap_beginning.wav\" # \"/Users/valentin/GoogleDrive/MusicEngine/\n",
    "sample_file = \"C:/Users/bre49823/GoogleDrive/MusicEngine/sample/sampleDt16bars102rap_middle.wav\"  # \"/Users/valentin/GoogleDrive/MusicEngine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8019968, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ff6a78f23155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m#### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mframes_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints_per_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfrequency_bins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints_per_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpoints_per_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Loop through all the frames/windows to which Fourier transform was applied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mtile\u001b[1;34m(A, reps)\u001b[0m\n\u001b[0;32m    912\u001b[0m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m//=\u001b[0m \u001b[0mdim_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Create acoustic fingerprints for all songs in the database\n",
    "########################################\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "song_fft_window = []\n",
    "band_0_25 = []\n",
    "band_26_50 = []\n",
    "band_51_75 = []\n",
    "band_76_100 = []\n",
    "band_101_125 = []\n",
    "band_126_150 = []\n",
    "band_151_175 = []\n",
    "band_176_200 = []\n",
    "band_201_225 = []\n",
    "band_226_250 = []\n",
    "\n",
    "#start_fingerprinting = timeit.default_timer()\n",
    "\n",
    "\n",
    "### Loop through all songs in the database for which I want to create a fingerprint\n",
    "for s in range(0, len(songs_list)):\n",
    "    \n",
    "    #### Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "    sample_rate, soundtrack_data = wavfile.read(songs_location + songs_list[s])\n",
    "\n",
    "    # If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "    if len(soundtrack_data.shape) == 2:\n",
    "        audio_signal = soundtrack_data.T[0]\n",
    "        #audio_signal = audio_signal[0:int(10 * sample_rate)]       # Just for testing purposes, keep only the first n seconds\n",
    "    else:\n",
    "        audio_signal = soundtrack_data\n",
    "        #audio_signal = audio_signal[0:int(10 * sample_rate)]\n",
    "\n",
    "    time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "    #### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "    # Calculate the length of each frame and the step for moving forward the FFT\n",
    "    frame_stride = round(frame_size - frame_overlap, 3)\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "    # Calculate the total number of frames\n",
    "    signal_length = len(audio_signal)\n",
    "    number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    # Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "    pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "    zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "    pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "    frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "    signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "    #### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "    signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "    #### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "    # The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "    signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "    signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "    # Calculate the power for each frame\n",
    "    signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "    \n",
    "    #### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "    # When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "    npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "    frequency_bins = 10\n",
    "    points_per_bin = npoints / frequency_bins\n",
    "\n",
    "    #### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "    frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "    # Loop through all the frames/windows to which Fourier transform was applied\n",
    "    for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "        # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "        fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "        # Split the results from the Fourier transform into the frequency bins created above\n",
    "        fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "        # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "        max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "        # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "        band_0_25.append(max_power[0])\n",
    "        band_26_50.append(max_power[1])\n",
    "        band_51_75.append(max_power[2])\n",
    "        band_76_100.append(max_power[3])\n",
    "        band_101_125.append(max_power[4])\n",
    "        band_126_150.append(max_power[5])\n",
    "        band_151_175.append(max_power[6])\n",
    "        band_176_200.append(max_power[7])\n",
    "        band_201_225.append(max_power[8])\n",
    "        band_226_250.append(max_power[9])\n",
    "\n",
    "        # Create an index which is a combination of song name and Fourier transform frame. This index tracks songs and frame sequence\n",
    "        # The number of records in this list should equal the number of records in the lists with frquency bands\n",
    "        fft_window = i            # A sequential number of the Fourier transform windows for each song\n",
    "        song_fft_window.append(songs_list[s].split(\".wav\")[0] + \"_\" + str(fft_window))\n",
    "\n",
    "\n",
    "#end_fingerprinting = timeit.default_timer()\n",
    "#print(end_fingerprinting - start_fingerprinting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                   Type        Data/Info\n",
      "------------------------------------------------\n",
      "audio_signal               ndarray     8019968: 8019968 elems, type `int16`, 16039936 bytes (15 Mb)\n",
      "band_0_25                  list        n=18184\n",
      "band_101_125               list        n=18184\n",
      "band_126_150               list        n=18184\n",
      "band_151_175               list        n=18184\n",
      "band_176_200               list        n=18184\n",
      "band_201_225               list        n=18184\n",
      "band_226_250               list        n=18184\n",
      "band_26_50                 list        n=18184\n",
      "band_51_75                 list        n=18184\n",
      "band_76_100                list        n=18184\n",
      "blackman                   function    <function blackman at 0x11cfbe668>\n",
      "dct                        function    <function dct at 0x11b713ed8>\n",
      "end                        float       1513526445.16\n",
      "end_fingerprinting         float       1513526933.29\n",
      "fft                        function    <function fft at 0x11b707ed8>\n",
      "fft_results                ndarray     250: 250 elems, type `float64`, 2000 bytes\n",
      "fft_results_tiled          ndarray     10x25: 250 elems, type `float64`, 2000 bytes\n",
      "fft_window                 int         18183\n",
      "frame_length               int         1103\n",
      "frame_overlap              float       0.015\n",
      "frame_size                 float       0.025\n",
      "frame_step                 int         441\n",
      "frame_stride               float       0.01\n",
      "frames_idx                 ndarray     10x25: 250 elems, type `int64`, 2000 bytes\n",
      "frequency_bins             int         10\n",
      "hamming                    function    <function hamming at 0x11cfbea28>\n",
      "hanning                    function    <function hann at 0x11cfbe8c0>\n",
      "i                          int         18183\n",
      "max_power                  list        n=10\n",
      "n_fft                      int         512\n",
      "np                         module      <module 'numpy' from '/Us<...>ages/numpy/__init__.pyc'>\n",
      "npoints                    int         250\n",
      "number_frames              int         18184\n",
      "os                         module      <module 'os' from '/Users<...>27/lib/python2.7/os.pyc'>\n",
      "pad_audio_length           int         8020247\n",
      "pad_signal                 ndarray     24060183: 24060183 elems, type `float64`, 192481464 bytes (183 Mb)\n",
      "plt                        module      <module 'matplotlib.pyplo<...>s/matplotlib/pyplot.pyc'>\n",
      "points_per_bin             int         25\n",
      "s                          int         0\n",
      "sample_file                str         /Users/valentin/GoogleDri<...>Dt16bars102rap_middle.wav\n",
      "sample_rate                int         44100\n",
      "signal                     module      <module 'scipy.signal' fr<...>ipy/signal/__init__.pyc'>\n",
      "signal_fft_transform       ndarray     18184x257: 4673288 elems, type `complex128`, 74772608 bytes (71 Mb)\n",
      "signal_fft_transform_abs   ndarray     18184x257: 4673288 elems, type `float64`, 37386304 bytes (35 Mb)\n",
      "signal_frames              ndarray     18184x1103: 20056952 elems, type `float64`, 160455616 bytes (153 Mb)\n",
      "signal_length              int         8019968\n",
      "signal_power               ndarray     18184x257: 4673288 elems, type `float64`, 37386304 bytes (35 Mb)\n",
      "song_fft_window            list        n=18184\n",
      "songs_list                 list        n=1\n",
      "songs_location             str         /Users/valentin/GoogleDrive/MusicEngine/wav/\n",
      "soundtrack_data            ndarray     8019968x2: 16039936 elems, type `int16`, 32079872 bytes (30 Mb)\n",
      "start                      float       1513526421.41\n",
      "start_fingerprinting       float       1513526931.37\n",
      "time                       ndarray     8019968: 8019968 elems, type `float64`, 64159744 bytes (61 Mb)\n",
      "timeit                     module      <module 'timeit' from '/U<...>ib/python2.7/timeit.pyc'>\n",
      "wavfile                    module      <module 'scipy.io.wavfile<...>es/scipy/io/wavfile.pyc'>\n",
      "x                          int         9\n",
      "zeros_vector               ndarray     16040215: 16040215 elems, type `float64`, 128321720 bytes (122 Mb)\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Create an acoustic fingerprint for the short song sample \n",
    "########################################\n",
    "\n",
    "# Create empty lists for song names with FFT frame and the frequency bands\n",
    "sample_0_25 = []\n",
    "sample_26_50 = []\n",
    "sample_51_75 = []\n",
    "sample_76_100 = []\n",
    "sample_101_125 = []\n",
    "sample_126_150 = []\n",
    "sample_151_175 = []\n",
    "sample_176_200 = []\n",
    "sample_201_225 = []\n",
    "sample_226_250 = []\n",
    "\n",
    "\n",
    "#### Read in the raw audio for the sample\n",
    "sample_rate, soundtrack_data = wavfile.read(sample_file)\n",
    "\n",
    "# If the audio is stereo (len(soundtrack_data.shape) == 2), take only one of the channels, otherwise if mono use as is\n",
    "if len(soundtrack_data.shape) == 2:\n",
    "    audio_signal = soundtrack_data.T[0]\n",
    "    audio_signal = audio_signal[0:int(2 * sample_rate)]       # keep only the first n seconds\n",
    "else:\n",
    "    audio_signal = soundtrack_data\n",
    "    audio_signal = audio_signal[0:int(2 * sample_rate)]\n",
    "\n",
    "time = np.arange(0, float(audio_signal.shape[0]), 1) / sample_rate\n",
    "\n",
    "\n",
    "#### Split the audio data into frames. Fourier Transform needs to be applied over short chunks of the raw audio data\n",
    "# Calculate the length of each frame and the step for moving forward the FFT\n",
    "frame_stride = round(frame_size - frame_overlap, 3)\n",
    "frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "\n",
    "# Calculate the total number of frames\n",
    "signal_length = len(audio_signal)\n",
    "number_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "# Pad the raw signal data with zeros to make sure that all frames have equal number of samples\n",
    "pad_audio_length = number_frames * frame_step + frame_length          # This number should be very close to the audio signal length. The difference is caused by the rounding in the calculation of number_frames\n",
    "zeros_vector = np.zeros((pad_audio_length + signal_length))\n",
    "pad_signal = np.append(audio_signal, zeros_vector)\n",
    "\n",
    "frames_idx = np.tile(np.arange(0, frame_length), (number_frames, 1)) + np.tile(np.arange(0, number_frames * frame_step, frame_step), (frame_length, 1)).T \n",
    "signal_frames = pad_signal[frames_idx.astype(np.int32, copy = False)]\n",
    "\n",
    "\n",
    "#### Create a window function for the Fourier transform. The windows are used for smoothing values of the raw signal in each time frame\n",
    "signal_frames *= np.hamming(frame_length)\n",
    "\n",
    "\n",
    "#### Calculate FFT (FFT is the implementation of the Discrete Fourier Transformation)\n",
    "# The FFT is symmetrical, and by using \"np.fft.rfft\" we only take the first half automatically. Otherwise, if we use \"np.fft.fft\" we'll need to take the first half only\n",
    "signal_fft_transform = np.fft.rfft(signal_frames, n = n_fft)\n",
    "signal_fft_transform_abs = np.absolute(signal_fft_transform)\n",
    "\n",
    "# Calculate the power for each frame\n",
    "signal_power = ((signal_fft_transform_abs ** 2) / n_fft)\n",
    "\n",
    "\n",
    "#### Define the length of each frequency bin by deciding how many bins I need and then chunk the output from FFT by each bin\n",
    "# When NFFT = 512, the result has 257 datapoints from which I subtract 7 to round the bins\n",
    "npoints = signal_fft_transform_abs[0].shape[0] - 7\n",
    "frequency_bins = 10\n",
    "points_per_bin = npoints / frequency_bins\n",
    "\n",
    "#### Create frequency bins from the indices of all frequencies in the range 0 - 250 (for NFFT = 512) in step of 25 \n",
    "frames_idx = np.tile(np.arange(0, points_per_bin, 1), (frequency_bins, 1)) + np.tile(np.arange(0, npoints, points_per_bin), (points_per_bin, 1)).T\n",
    "\n",
    "# Loop through all the frames/windows to which Fourier transform was applied\n",
    "for i in range(0, signal_fft_transform_abs.shape[0]):\n",
    "\n",
    "    # Limit the output from the Fourier transform only to the first 250 points (for NFFT = 512)\n",
    "    fft_results = signal_fft_transform_abs[i, 0:npoints]\n",
    "\n",
    "    # Split the results from the Fourier transform into the frequency bins created above\n",
    "    fft_results_tiled = fft_results[frames_idx]\n",
    "\n",
    "    # Calculate the maximum power in each bin. This returns a list with the maximum power for each frequency bin in the window frames of the audio signal\n",
    "    max_power = [max(fft_results_tiled[x]) for x in range(0, frames_idx.shape[0])]\n",
    "\n",
    "    # Append the maximum power from each frequency band to the appropriate frequency band lists\n",
    "    sample_0_25.append(max_power[0])\n",
    "    sample_26_50.append(max_power[1])\n",
    "    sample_51_75.append(max_power[2])\n",
    "    sample_76_100.append(max_power[3])\n",
    "    sample_101_125.append(max_power[4])\n",
    "    sample_126_150.append(max_power[5])\n",
    "    sample_151_175.append(max_power[6])\n",
    "    sample_176_200.append(max_power[7])\n",
    "    sample_201_225.append(max_power[8])\n",
    "    sample_226_250.append(max_power[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    100.0\n",
      "1    100.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>band_0_25</th>\n",
       "      <th>band_26_50</th>\n",
       "      <th>band_51_75</th>\n",
       "      <th>band_76_100</th>\n",
       "      <th>band_101_125</th>\n",
       "      <th>band_126_150</th>\n",
       "      <th>band_151_175</th>\n",
       "      <th>band_176_200</th>\n",
       "      <th>band_201_225</th>\n",
       "      <th>band_226_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt16bars102rap - Copy</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dt16bars102rap</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    song  band_0_25  band_26_50  band_51_75  band_76_100  \\\n",
       "0  dt16bars102rap - Copy        198         198         198          198   \n",
       "1         dt16bars102rap        198         198         198          198   \n",
       "\n",
       "   band_101_125  band_126_150  band_151_175  band_176_200  band_201_225  \\\n",
       "0           198           198           198           198           198   \n",
       "1           198           198           198           198           198   \n",
       "\n",
       "   band_226_250  \n",
       "0           198  \n",
       "1           198  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "# Match the fingerprint of the sample to the database and return the results\n",
    "# Determine which match is the actual\n",
    "########################################\n",
    "\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "database_songs_list = [x.split(\".wav\")[0] for x in os.listdir(songs_location)]\n",
    "final_match_df = pd.DataFrame(data = database_songs_list, columns = [\"song\"])\n",
    "\n",
    "sample_0_25 = set(sample_0_25)\n",
    "sample_26_50 = set(sample_26_50)\n",
    "sample_51_75 = set(sample_51_75)\n",
    "sample_76_100 = set(sample_76_100)\n",
    "sample_101_125 = set(sample_101_125)\n",
    "sample_126_150 = set(sample_126_150)\n",
    "sample_151_175 = set(sample_151_175)\n",
    "sample_176_200 = set(sample_176_200)\n",
    "sample_201_225 = set(sample_201_225)\n",
    "sample_226_250 = set(sample_226_250)\n",
    "\n",
    "sample_freq_bands_idx = [sample_0_25, sample_26_50, sample_51_75, sample_76_100, sample_101_125, sample_126_150, sample_151_175, sample_176_200, sample_201_225, sample_226_250]\n",
    "database_freq_bands_idx = [band_0_25, band_26_50, band_51_75, band_76_100, band_101_125, band_126_150, band_151_175, band_176_200, band_201_225, band_226_250]\n",
    "database_freq_bands_list = [\"band_0_25\", \"band_26_50\", \"band_51_75\", \"band_76_100\", \"band_101_125\", \"band_126_150\", \"band_151_175\", \"band_176_200\", \"band_201_225\", \"band_226_250\"]\n",
    "\n",
    "\n",
    "for j in range(0, len(sample_freq_bands_idx)):\n",
    "\n",
    "    match_idx = [i for i, item in enumerate(database_freq_bands_idx[j]) if item in sample_freq_bands_idx[j]]\n",
    "    song_match_list = [song_fft_window[i].split(\"_\")[0] for i in match_idx]\n",
    "    \n",
    "    count_match_occurences = dict(Counter(song_match_list))\n",
    "    sorted_match_occurences = sorted(count_match_occurences.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    summary_match_df = pd.DataFrame.from_records(sorted_match_occurences, columns = [\"song\", database_freq_bands_list[j]])\n",
    "    final_match_df = pd.merge(final_match_df, summary_match_df, on = \"song\")\n",
    "\n",
    "# Print out the index of songs that are a match\n",
    "print ((final_match_df.sum(axis = 1) / 1980) * 100)\n",
    "final_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
