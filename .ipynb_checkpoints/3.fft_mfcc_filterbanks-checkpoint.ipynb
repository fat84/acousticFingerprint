{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an implementation of an FFT on a song\n",
    "The inspiration for this implementation came from http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import blackman, hanning, hamming\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Location of the song\n",
    "files_location = \"/Users/valentin/Google Drive/dt_16bars_102rap.wav\" # \"C:/Users/bre49823/Google Drive/dt_16bars_102rap.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data and create a plot of the raw audio file\n",
    "The raw data is in the time domain and using Fourier transform I'll convert it to frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "sample_rate, soundtrack_data = wavfile.read(files_location)\n",
    "audio_data = soundtrack_data.T[0]                      # this is a two channel soundtrack, get only one of the tracks\n",
    "audio_data = audio_data[0:int(10 * sample_rate)]      # keep only the first 10 seconds\n",
    "\n",
    "time = np.arange(0, float(audio_data.shape[0]), 1) / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw signal - amplitude (or loudness) over time\n",
    "plt.figure(figsize = (25, 15))\n",
    "plt.plot(time, audio_data, linewidth = 0.1, alpha = 2, color='#ff0000')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Audio signal in the time domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply pre-emphasis filter to the raw audio data\n",
    "This makes the signal cleaner with less noise. It is done to balance the frequency spectrum since high frequencies have lower magnitude than lower frequncies.\n",
    "<br>\n",
    "The pre-emphasis filter is y(t) = x(t) - emphasis_coeff * (t - 1)\n",
    "<br>\n",
    "Typical values for the emphasis coefficients are 0.95 and 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-emphasis filter\n",
    "emphasis_coeff = 0.95\n",
    "emphasized_audio_data = np.append(audio_data[0], audio_data[1:] - emphasis_coeff * audio_data[:-1])\n",
    "\n",
    "# Plot the emphasized signal\n",
    "plt.figure(figsize = (25, 15))\n",
    "plt.plot(time, emphasized_audio_data, linewidth = 0.1, alpha = 2, color='#ff0000')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Pre-emphaised audio signal in the time domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Split the audio data into frames\n",
    "When we apply Fourier transform it needs to be applied over short chunks of the raw audio data. The frequencies in the audio signal change over time, and if we do the transformation using the whole length of the song, we would lose these changes in frequencies.\n",
    "<n>\n",
    "The frames need to be overlapping. According to Haytham Fayek (http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html), frame sizes for speech processing range from 20ms to 40ms with 50%(+/-10%) overlap between consecutive frames.\n",
    "<n>\n",
    "I'll use 25ms for the frame length and 15ms overlap. The difference between the frame length and the overlap is called a stride (i.e. stride = frame length - overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_length, frame_stride = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a window for the FFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate FFT and power spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot spectrogram\n",
    "plt.imshow(np.transpose(ims), origin = \"lower\", aspect = \"auto\", cmap = colormap, interpolation = \"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create filter banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
