{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an implementation of an FFT on a song\n",
    "The inspiration for this implementation came from http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, dct\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import blackman, hanning, hamming\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Location of the song\n",
    "files_location = \"C:/Users/bre49823/Google Drive/dt_16bars_102rap.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data and create a plot of the raw audio file\n",
    "The raw data is in the time domain and using Fourier transform I'll convert it to frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the raw audio data and get the sample rate (in samples/sec)\n",
    "sample_rate, soundtrack_data = wavfile.read(files_location)\n",
    "audio_data = soundtrack_data.T[0]                      # this is a two channel soundtrack, get only one of the tracks\n",
    "audio_data = audio_data[0:int(10 * sample_rate)]      # keep only the first 10 seconds\n",
    "\n",
    "time = np.arange(0, float(audio_data.shape[0]), 1) / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw signal - amplitude (or loudness) over time\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.plot(time, audio_data, linewidth = 0.1, alpha = 2, color='#ff0000')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Audio signal in the time domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-emphasis\n",
    "Apply pre-emphasis to the audio data. This makes the signal cleaner with less noise. It is done to balance the frequency spectrum since high frequencies have lower magnitude than lower frequncies.\n",
    "<br>\n",
    "The pre-emphasis filter is y(t) = x(t) - emphasis_coeff * (t - 1)\n",
    "<br>\n",
    "Typical values for the emphasis coefficients are 0.95 and 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-emphasis filter\n",
    "emphasis_coeff = 0.95\n",
    "emphasized_audio_data = np.append(audio_data[0], audio_data[1:] - emphasis_coeff * audio_data[:-1])\n",
    "\n",
    "# Plot the emphasized signal\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.plot(time, emphasized_audio_data, linewidth = 0.1, alpha = 2, color='#ff0000')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Pre-emphaised audio signal in the time domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Framing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a window for the FFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate FFT and power spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot spectrogram\n",
    "plt.imshow(np.transpose(ims), origin = \"lower\", aspect = \"auto\", cmap = colormap, interpolation = \"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create filter banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
